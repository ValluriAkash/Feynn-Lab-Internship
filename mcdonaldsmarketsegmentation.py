# -*- coding: utf-8 -*-
"""McdonaldsMarketsegmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jxfmuZMDQQIMGZqxXZtDOajptr4w9Q-o
"""

import pandas as pd

mcdonalds = pd.read_csv('mcdonalds.csv')

column_names = mcdonalds.columns.tolist()
print(column_names)

import numpy as np

mcdonalds = pd.read_csv('mcdonalds.csv')
print(mcdonalds.shape)

print(mcdonalds.head(3))

MD_x = mcdonalds.iloc[:, 0:11].values
MD_x = (MD_x == "Yes").astype(int)
print(np.round(np.mean(MD_x, axis=0), 2))

from sklearn.decomposition import PCA

MD_pca = PCA().fit(MD_x)
print(MD_pca.explained_variance_ratio_)

print(round(MD_x.pca, 1))

import matplotlib.pyplot as plt
pca = PCA()
MD_pca = pca.fit_transform(MD_x)
plt.scatter(MD_pca[:, 0], MD_pca[:, 1], color="grey")
origin = pca.mean_
components = pca.components_.T * pca.singular_values_
for i in range(components.shape[0]):
    plt.arrow(origin[0], origin[1], components[i, 0], components[i, 1], color='red', alpha=0.5)

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('PCA Scatter Plot')
plt.show()

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
import matplotlib.pyplot as plt

np.random.seed(1234)

# Assuming data is your data matrix

# Initialize variables
num_segments = range(2, 9)
n_rep = 10
adjusted_rand_indices = []

for k in num_segments:
    rand_indices = []
    for rep in range(n_rep):
        kmeans = KMeans(n_clusters=k)
        labels = kmeans.fit_predict(MD_x)

        # Compute adjusted Rand index
        rand_index = adjusted_rand_score(labels, labels)
        rand_indices.append(rand_index)

    # Compute mean of adjusted Rand indices for the current number of segments
    mean_rand_index = np.mean(rand_indices)
    adjusted_rand_indices.append(mean_rand_index)

# Plotting
plt.plot(num_segments, adjusted_rand_indices)
plt.xlabel("Number of segments")
plt.ylabel("Adjusted Rand index")
plt.show()

import numpy as np
from sklearn.cluster import KMeans
from sklearn.metrics import adjusted_rand_score
from sklearn.utils import resample
import matplotlib.pyplot as plt

np.random.seed(1234)

# Assuming data is your data matrix

# Initialize variables
num_segments = range(2, 9)
n_rep = 10
n_boot = 100
adjusted_rand_indices = []

for k in num_segments:
    rand_indices = []
    for rep in range(n_rep):
        boot_indices = resample(range(len(MD_x)), n_samples=len(MD_x), replace=True)
        boot_data = MD_x[boot_indices]

        kmeans = KMeans(n_clusters=k)
        labels = kmeans.fit_predict(boot_data)

        # Compute adjusted Rand index
        rand_index = adjusted_rand_score(boot_indices, labels)
        rand_indices.append(rand_index)

    # Compute mean of adjusted Rand indices for the current number of segments
    mean_rand_index = np.mean(rand_indices)
    adjusted_rand_indices.append(mean_rand_index)

# Plotting
plt.plot(num_segments, adjusted_rand_indices)
plt.xlabel("Number of segments")
plt.ylabel("Adjusted Rand index")
plt.show()

import matplotlib.pyplot as plt

# Assuming MD_km28 is the array of data for the "4" segment

# Histogram plot
plt.hist(MD_x, bins=20, range=(0, 1))
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.title('Histogram of Segment "4"')
plt.xlim(0, 1)
plt.show()

import numpy as np
import matplotlib.pyplot as plt

# Assuming MD_km28 is the array of data for the "4" segment
MD_k4 = MD_x

# Compute segment stability values
segment_stability = []
for segment in range(1, len(MD_k4) + 1):
    segment_stability.append(np.mean(MD_k4[segment - 1]))

# Plot segment stability
segment_numbers = range(1, len(MD_k4) + 1)
plt.plot(segment_numbers, segment_stability)
plt.xlabel('Segment number')
plt.ylabel('Segment stability')
plt.ylim(0, 1)
plt.title('Segment Stability')
plt.show()

import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.mixture import GaussianMixture

# Assuming MD_x is the input data

# Set the random seed
np.random.seed(1234)

# StepFlexmix
k_values = range(2, 9)
AIC_values = []
BIC_values = []
ICL_values = []

for k in k_values:
    # K-means clustering
    kmeans = KMeans(n_clusters=k, random_state=1234)
    kmeans.fit(MD_x)
    
    # Mixture model
    mixture = GaussianMixture(n_components=k, random_state=1234)
    mixture.fit(MD_x)
    
    # Compute information criteria
    AIC_values.append(mixture.aic(MD_x))
    BIC_values.append(mixture.bic(MD_x))
    ICL_values.append(mixture.lower_bound_)

# Plot information criteria
plt.plot(k_values, AIC_values, label='AIC')
plt.plot(k_values, BIC_values, label='BIC')
plt.plot(k_values, ICL_values, label='ICL')
plt.xlabel('Number of Segments')
plt.ylabel('Value of Information Criteria')
plt.title('Information Criteria')
plt.legend()
plt.show()

# Get the model for 4 segments
num_segments = 4
mixture_model = GaussianMixture(n_components=num_segments, random_state=1234)
mixture_model.fit(MD_x)

# K-means clustering for 4 segments
kmeans_model = KMeans(n_clusters=num_segments, random_state=1234)
kmeans_model.fit(MD_k4)

# Table of cluster assignments
table_result = {
    'kmeans': kmeans_model.labels_,
    'mixture': mixture_model.predict(MD_x)
}

print(table_result)

import pandas as pd
import statsmodels.formula.api as smf

# Assuming 'mcdonalds' is the DataFrame containing the 'Like' variable

# Remove non-numeric values from the 'Like' column
mcdonalds['Like'] = mcdonalds['Like'].str.replace('[^0-9-]', '', regex=True)

# Reverse the frequency table
freq_table_rev = mcdonalds['Like'].value_counts().sort_index(ascending=False)

print(freq_table_rev)

# Create the 'Like.n' variable
mcdonalds['Like.n'] = 6 - pd.to_numeric(mcdonalds['Like'])

# Create the frequency table for 'Like.n'
freq_table_like_n = mcdonalds['Like.n'].value_counts().sort_index()

print(freq_table_like_n)

# Create the formula for regression model
variable_names = mcdonalds.columns[1:12]  # Assuming the independent variables start from column index 1
f = 'Like.n ~ ' + ' + '.join(variable_names)

# Create the formula object
print(f)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import dendrogram, linkage
from sklearn.preprocessing import StandardScaler

# Assuming 'MD.x' is the data matrix

# Perform hierarchical clustering
dist_matrix = pdist(np.transpose(MD_x))
linkage_matrix = linkage(dist_matrix, method='average')
dendrogram(linkage_matrix)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Segments')
plt.ylabel('Distance')
plt.show()

# Get the ordering of segmentation variables from the hierarchical clustering
segment_order = linkage_matrix[:, [0, 1]].astype(int)

# Reorder the segment order in descending order
segment_order = segment_order[::-1]

# Assuming 'MD.k4' is the clustering result with 4 segments
shade_variables = ['Variable1', 'Variable2']  # Example list of marker variables to be highlighted

# Plot the segment profile plot with highlighted marker variables
plt.figure(figsize=(10, 6))
for segment in range(4):
    segment_profile = MD_x[segment]
    color = 'blue'
    if segment_profile in shade_variables:
        color = 'red'
    plt.bar(range(len(segment_profile)), segment_profile, color=color)
plt.xlabel('Segment Variables')
plt.ylabel('Segment Profile')
plt.title('Segment Profile Plot')
plt.xticks(range(len(segment_order)), segment_order)
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Assuming 'MD.x' is the data matrix
# Assuming 'MD.k4' is the clustering result with 4 segments

# Perform PCA on the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(MD_x)
pca = PCA(n_components=2)
MD_pca = pca.fit_transform(X_scaled)

# Plot the clustering result projected onto the principal components
plt.figure(figsize=(10, 6))
for segment in range(4):
    segment_indices = np.where(MD_x == segment)[0]
    plt.scatter(MD_pca[segment_indices, 0], MD_pca[segment_indices, 1], label=f"Segment {segment+1}")
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('Clustering Result Projected onto Principal Components')
plt.legend()
plt.show()

# Calculate the projection axes
proj_axes = pca.components_

# Print the projection axes
print('Projection Axes:')
print(proj_axes)